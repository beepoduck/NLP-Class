{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW6.1 Text classification with w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same data set Triage as last two weeks. Here is what's in this assignment. \n",
    "\n",
    "1. we will explore text classification with pre-trained w2v embeddings with logistic regression. \n",
    "\n",
    "2. we will explore text classification with w2v embeddings trained on the Triage training dataset and then test it on the dev dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Using pre-trained w2v embeddings for text classification\n",
    "\n",
    "For data loading, you should use the same code as last time so that you can obtain train_text, train_label, dev_text, dev_label, etc. \n",
    "\n",
    "To get pretrained w2v embeddings, we can use the ```gensim``` library. You can do \n",
    "\n",
    "```!pip install gensim```\n",
    "\n",
    "to get it first. \n",
    "\n",
    "One you installed the library, you can take a look at which pretrained embeddings are available for your to download. \n",
    "\n",
    "```\n",
    "import gensim.downloader\n",
    "#Show all available models in gensim-data\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "\n",
    "```\n",
    "\n",
    "You should see a list of available pretrained embeddings like this: \n",
    "\n",
    "```\n",
    "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n",
    "```\n",
    "\n",
    "We recommend trying out a few, like the 'glove-wiki-gigaword-300' and 'word2vec-google-news-300'. To download the embeddings: \n",
    "\n",
    "```\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "```\n",
    "\n",
    "Once you downloaded it into a variable, you can do many things. For instance, you can find the most similar words to a query word you put in: \n",
    "\n",
    "```\n",
    "glove_vectors.most_similar('how')\n",
    "```\n",
    "\n",
    "You can also look at the embedding of a word: \n",
    "\n",
    "```\n",
    "word = \"how\"\n",
    "word_embedding = glove_vectors[word]\n",
    "\n",
    "```\n",
    "\n",
    "In tfidf, a sentence or document is naturally represented as a vector by the vocabulary based vectors. However, in w2v, you have a vector for each word, but not a sentence (alternatively, you can use something called doc2vec to directly encode a sentence). The most common way to get a sentence vector from word vectors is just to go through each word, get their embeddings and finally take an average of all word embeddings. If each word is a 300-d vector, then the final sentence vector is also 300-d. \n",
    "\n",
    "### Task 1: Write a ```get_sentence_embedding()``` function. \n",
    "\n",
    "First, you need to write a function to get sentence embeddings from all words. Note that when you look up a word embedding in the pretrained w2v, there is no guarantee that that word is in the w2v dictionary. If not, then you will get an error when you look at that word. In your code, you should build in error handling to take care of this situation. If a word is not present in the dictionary, you should initialize it with a 300-d zero vector using ```numpy.zeros()```. \n",
    "\n",
    "For this task let's use the pre-trained google news 300-d vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:31:57: loading projection weights from /Users/asze01/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n",
      "INFO - 17:32:20: KeyedVectors lifecycle event {'msg': 'loaded (3000000, 300) matrix of type float32 from /Users/asze01/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-10-19T17:32:20.689223', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "import numpy as np\n",
    "def get_sentence_embedding(sentence:str,glove_vectors)->np.ndarray:\n",
    "    \"\"\"\n",
    "    function to get embedding of a sentence from the words in it using w2v\n",
    "\n",
    "    args:\n",
    "        sentence: the input sentence to compute embeddings from \n",
    "        glove_vectors: the pretrained w2v object where you can look up word embeddings\n",
    "    returns:\n",
    "        a numpy ndarray with the same dimension as the pretrained w2v embeddings\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    embeddings = []\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            embedding = glove_vectors[word]\n",
    "        except KeyError:  \n",
    "            embedding = np.zeros(300) \n",
    "        embeddings.append(embedding)\n",
    "\n",
    "    if not embeddings:\n",
    "        return np.zeros(300)\n",
    "\n",
    "    avg_embedding = np.mean(embeddings, axis=0)\n",
    "\n",
    "    return avg_embedding\n",
    "\n",
    "import gensim.downloader\n",
    "glove_vectors = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: encode your input sentences from train and test portion of the Triage dataset into vector representations \n",
    "\n",
    "Last week we saw how to use tf-idf vectors to represent sentences and use them in a classifier. Here we just need to similarly turn all training and dev sentences into vectors, but using w2v. \n",
    "\n",
    "Make use of the function above and go through all sentences in your train data and dev data. One possibility is that all of the words in a sentence may be absent from your pretrained w2v dictionary. In that case, it would just come out as a zero vector for the whole sentence, which may not be ideal but let's keep it simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import load_data, Dataset, Example\n",
    "import numpy as np\n",
    "dataset = load_data(\"./data/triage\")\n",
    "\n",
    "\n",
    "def get_data(split:list[Example])->(list[str],list[int]):\n",
    "    \"\"\"\n",
    "    massage the data into a format consistent with the input type required by CountVectorizer or tfidfVectorizer. \n",
    "\n",
    "    args \n",
    "        split: pass in the split, which should be either dataset.train or dataset.dev\n",
    "\n",
    "    returns: \n",
    "        text: list of sentences\n",
    "        labels: list of labels  \n",
    "\n",
    "    \"\"\"\n",
    "    text = [' '.join(example.words) for example in split]\n",
    "    labels = [example.label for example in split]\n",
    "    return text, labels\n",
    "    \n",
    "\n",
    "# Load data using get_data\n",
    "train_text, train_label = get_data(dataset.train)\n",
    "dev_text, dev_label = get_data(dataset.dev)\n",
    "\n",
    "# Write code to go through all sentences in the train and dev data respectively \n",
    "# and encode them into vectors using the function you wrote above with w2v\n",
    "# Make sure the final matrix for your training set and dev set are represented in\n",
    "# numpy arrays, not list of lists. \n",
    "\n",
    "train_embeddings = [get_sentence_embedding(sentence, glove_vectors) for sentence in train_text]\n",
    "\n",
    "dev_embeddings = [get_sentence_embedding(sentence, glove_vectors) for sentence in dev_text]\n",
    "\n",
    "train_embeddings_array = np.array(train_embeddings)\n",
    "dev_embeddings_array = np.array(dev_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Logistic regression text classification with w2v\n",
    "\n",
    "Feed your w2v encoded train data into the logistic regression classifier you worked with last week, except this time you should use the scikit-learn built-in function of logistic regression. Report the accuracy for train and dev datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 76.43%\n",
      "Development Accuracy: 76.25%\n"
     ]
    }
   ],
   "source": [
    "# code for logistic regression with scikit-learn library.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "model.fit(train_embeddings_array, train_label)\n",
    "train_predictions = model.predict(train_embeddings_array)\n",
    "dev_predictions = model.predict(dev_embeddings_array)\n",
    "\n",
    "train_accuracy = accuracy_score(train_label, train_predictions)\n",
    "dev_accuracy = accuracy_score(dev_label, dev_predictions)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Development Accuracy: {dev_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Train your own w2v embeddimgs on the Triage training data and test it on the dev data\n",
    "\n",
    "In this part we will train our w2v model based on the training dataset. First, you can read through the gensim package tutorial. Pay special attention to the ```training parameters section``` to understand the parameters in the ```Word2Vec``` function below. \n",
    "\n",
    "Assuming you have the ```train_text``` variable set up above, which is a list of sentences, we would still need to break each sentence into a list of words. In the below code, we first do that, then take the three steps to train a w2v model:\n",
    "\n",
    "1. initialize model with ```Word2Vec()```\n",
    "2. build your vocab\n",
    "3. train the model. \n",
    "\n",
    "### Task 3: train w2v model with default parameters\n",
    "\n",
    "using the code below, and then use your above code to feed your text training data and dev data to your logistic regression model with this new trained w2v dictionary. Note that to load the embeddings for a word, you need to look it up by: \n",
    "\n",
    "```word_emb = w2v_vector.wv[word]```\n",
    "\n",
    "Which is a little different from the pre-trained model. \n",
    "\n",
    "After training your logistic regression model, report accuracy for both training and dev data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 17:32:28: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=0, vector_size=300, alpha=0.025>', 'datetime': '2023-10-19T17:32:28.035949', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO - 17:32:28: collecting all words and their counts\n",
      "INFO - 17:32:28: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 17:32:28: PROGRESS: at sentence #10000, processed 241164 words, keeping 21647 word types\n",
      "INFO - 17:32:28: PROGRESS: at sentence #20000, processed 481958 words, keeping 31476 word types\n",
      "INFO - 17:32:28: collected 32305 word types from a corpus of 506449 raw words and 21046 sentences\n",
      "INFO - 17:32:28: Creating a fresh vocabulary\n",
      "INFO - 17:32:28: Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 4573 unique words (14.16% of original 32305, drops 27732)', 'datetime': '2023-10-19T17:32:28.104689', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 17:32:28: Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 450335 word corpus (88.92% of original 506449, drops 56114)', 'datetime': '2023-10-19T17:32:28.105630', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 17:32:28: deleting the raw counts dictionary of 32305 items\n",
      "INFO - 17:32:28: sample=0.001 downsamples 43 most-common words\n",
      "INFO - 17:32:28: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 345987.03433684784 word corpus (76.8%% of prior 450335)', 'datetime': '2023-10-19T17:32:28.115589', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 17:32:28: estimated required memory for 4573 words and 300 dimensions: 13261700 bytes\n",
      "INFO - 17:32:28: resetting layer weights\n",
      "INFO - 17:32:28: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-10-19T17:32:28.136526', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "INFO - 17:32:28: Word2Vec lifecycle event {'msg': 'training model with 8 workers on 4573 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2023-10-19T17:32:28.137003', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 17:32:28: EPOCH 0: training on 506449 raw words (346064 effective words) took 0.1s, 2350489 effective words/s\n",
      "INFO - 17:32:28: EPOCH 1: training on 506449 raw words (345818 effective words) took 0.2s, 2240193 effective words/s\n",
      "INFO - 17:32:28: EPOCH 2: training on 506449 raw words (345893 effective words) took 0.2s, 2266406 effective words/s\n",
      "INFO - 17:32:28: EPOCH 3: training on 506449 raw words (345968 effective words) took 0.2s, 2223516 effective words/s\n",
      "INFO - 17:32:28: EPOCH 4: training on 506449 raw words (346101 effective words) took 0.1s, 2361300 effective words/s\n",
      "INFO - 17:32:29: EPOCH 5: training on 506449 raw words (346027 effective words) took 0.2s, 2243711 effective words/s\n",
      "INFO - 17:32:29: EPOCH 6: training on 506449 raw words (346100 effective words) took 0.2s, 2287856 effective words/s\n",
      "INFO - 17:32:29: EPOCH 7: training on 506449 raw words (346057 effective words) took 0.1s, 2348196 effective words/s\n",
      "INFO - 17:32:29: EPOCH 8: training on 506449 raw words (345808 effective words) took 0.1s, 2380137 effective words/s\n",
      "INFO - 17:32:29: EPOCH 9: training on 506449 raw words (345981 effective words) took 0.2s, 2269555 effective words/s\n",
      "INFO - 17:32:29: EPOCH 10: training on 506449 raw words (346134 effective words) took 0.1s, 2348900 effective words/s\n",
      "INFO - 17:32:29: EPOCH 11: training on 506449 raw words (346228 effective words) took 0.2s, 2239196 effective words/s\n",
      "INFO - 17:32:30: EPOCH 12: training on 506449 raw words (346137 effective words) took 0.1s, 2360629 effective words/s\n",
      "INFO - 17:32:30: EPOCH 13: training on 506449 raw words (346066 effective words) took 0.2s, 2301320 effective words/s\n",
      "INFO - 17:32:30: EPOCH 14: training on 506449 raw words (345857 effective words) took 0.1s, 2376645 effective words/s\n",
      "INFO - 17:32:30: EPOCH 15: training on 506449 raw words (345755 effective words) took 0.1s, 2427950 effective words/s\n",
      "INFO - 17:32:30: EPOCH 16: training on 506449 raw words (346012 effective words) took 0.1s, 2404534 effective words/s\n",
      "INFO - 17:32:30: EPOCH 17: training on 506449 raw words (345757 effective words) took 0.2s, 2224557 effective words/s\n",
      "INFO - 17:32:31: EPOCH 18: training on 506449 raw words (345694 effective words) took 0.1s, 2511355 effective words/s\n",
      "INFO - 17:32:31: EPOCH 19: training on 506449 raw words (345895 effective words) took 0.1s, 2306017 effective words/s\n",
      "INFO - 17:32:31: EPOCH 20: training on 506449 raw words (345996 effective words) took 0.1s, 2370629 effective words/s\n",
      "INFO - 17:32:31: EPOCH 21: training on 506449 raw words (345983 effective words) took 0.1s, 2411289 effective words/s\n",
      "INFO - 17:32:31: EPOCH 22: training on 506449 raw words (346060 effective words) took 0.2s, 2305621 effective words/s\n",
      "INFO - 17:32:31: EPOCH 23: training on 506449 raw words (345960 effective words) took 0.2s, 2245328 effective words/s\n",
      "INFO - 17:32:31: EPOCH 24: training on 506449 raw words (345928 effective words) took 0.1s, 2388309 effective words/s\n",
      "INFO - 17:32:32: EPOCH 25: training on 506449 raw words (345796 effective words) took 0.2s, 2271897 effective words/s\n",
      "INFO - 17:32:32: EPOCH 26: training on 506449 raw words (346405 effective words) took 0.2s, 2278113 effective words/s\n",
      "INFO - 17:32:32: EPOCH 27: training on 506449 raw words (346043 effective words) took 0.1s, 2379121 effective words/s\n",
      "INFO - 17:32:32: EPOCH 28: training on 506449 raw words (345919 effective words) took 0.1s, 2319757 effective words/s\n",
      "INFO - 17:32:32: EPOCH 29: training on 506449 raw words (346028 effective words) took 0.1s, 2377336 effective words/s\n",
      "INFO - 17:32:32: EPOCH 30: training on 506449 raw words (345931 effective words) took 0.1s, 2407655 effective words/s\n",
      "INFO - 17:32:33: EPOCH 31: training on 506449 raw words (346229 effective words) took 0.1s, 2324996 effective words/s\n",
      "INFO - 17:32:33: Word2Vec lifecycle event {'msg': 'training on 16206368 raw words (11071630 effective words) took 4.9s, 2261539 effective words/s', 'datetime': '2023-10-19T17:32:33.032888', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "import logging \n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 8\n",
    "W2V_EPOCH = 32\n",
    "W2V_MIN_COUNT = 10\n",
    "\n",
    "sentences = [s.split() for s in train_text]\n",
    "\n",
    "w2v_model = Word2Vec(vector_size=W2V_SIZE, \n",
    "                    window=W2V_WINDOW, \n",
    "                    min_count=W2V_MIN_COUNT, \n",
    "                    workers=8)\n",
    "\n",
    "w2v_model.build_vocab(sentences, progress_per=10000)\n",
    "\n",
    "w2v_model.train(sentences, total_examples=len(sentences), epochs=W2V_EPOCH, report_delay=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 75.76%\n",
      "Dev Accuracy: 74.70%\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# to train a logistic regression model with your new w2v embeddings\n",
    "# report accuracy for training and dev data\n",
    "\n",
    "# code for logistic regression with scikit-learn library.\n",
    "\n",
    "def get_sentence_embedding2(sentence, w2v_model):\n",
    "    \"\"\"\n",
    "    This function takes a sentence and w2v model as input.\n",
    "    It returns the average embedding of the sentence.\n",
    "    \"\"\"\n",
    "    words = sentence.split()\n",
    "    embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in w2v_model.wv:\n",
    "            embeddings.append(w2v_model.wv[word])\n",
    "    \n",
    "    if not embeddings:\n",
    "        return np.zeros(W2V_SIZE)\n",
    "    \n",
    "    sentence_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "X_train = np.array([get_sentence_embedding2(sentence, w2v_model) for sentence in train_text])\n",
    "X_dev = np.array([get_sentence_embedding2(sentence, w2v_model) for sentence in dev_text])\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000).fit(X_train, train_label)\n",
    "\n",
    "train_pred = clf.predict(X_train)\n",
    "dev_pred = clf.predict(X_dev)\n",
    "\n",
    "train_acc = accuracy_score(train_label, train_pred)\n",
    "dev_acc = accuracy_score(dev_label, dev_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc*100:.2f}%\")\n",
    "print(f\"Dev Accuracy: {dev_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1: play with hyperparameters\n",
    "\n",
    "Change the hyperparameters such as vector_size, window, min_count, etc., and train your w2v model again. Does the accuracy change? Report your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:22:18: collecting all words and their counts\n",
      "INFO - 18:22:18: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:22:18: PROGRESS: at sentence #10000, processed 241164 words, keeping 21647 word types\n",
      "INFO - 18:22:18: PROGRESS: at sentence #20000, processed 481958 words, keeping 31476 word types\n",
      "INFO - 18:22:18: collected 32305 word types from a corpus of 506449 raw words and 21046 sentences\n",
      "INFO - 18:22:18: Creating a fresh vocabulary\n",
      "INFO - 18:22:18: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 7451 unique words (23.06% of original 32305, drops 24854)', 'datetime': '2023-10-19T18:22:18.482137', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:18: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 469185 word corpus (92.64% of original 506449, drops 37264)', 'datetime': '2023-10-19T18:22:18.482751', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:18: deleting the raw counts dictionary of 32305 items\n",
      "INFO - 18:22:18: sample=0.001 downsamples 41 most-common words\n",
      "INFO - 18:22:18: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 366705.58609343914 word corpus (78.2%% of prior 469185)', 'datetime': '2023-10-19T18:22:18.496471', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:18: estimated required memory for 7451 words and 100 dimensions: 9686300 bytes\n",
      "INFO - 18:22:18: resetting layer weights\n",
      "INFO - 18:22:18: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-10-19T18:22:18.522612', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "INFO - 18:22:18: Word2Vec lifecycle event {'msg': 'training model with 8 workers on 7451 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-10-19T18:22:18.522892', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 18:22:18: EPOCH 0: training on 506449 raw words (366558 effective words) took 0.1s, 3452968 effective words/s\n",
      "INFO - 18:22:18: EPOCH 1: training on 506449 raw words (366751 effective words) took 0.1s, 3479216 effective words/s\n",
      "INFO - 18:22:18: EPOCH 2: training on 506449 raw words (366484 effective words) took 0.1s, 3629251 effective words/s\n",
      "INFO - 18:22:18: EPOCH 3: training on 506449 raw words (366663 effective words) took 0.1s, 3252728 effective words/s\n",
      "INFO - 18:22:19: EPOCH 4: training on 506449 raw words (367058 effective words) took 0.1s, 3553716 effective words/s\n",
      "INFO - 18:22:19: Word2Vec lifecycle event {'msg': 'training on 2532245 raw words (1833514 effective words) took 0.6s, 3328970 effective words/s', 'datetime': '2023-10-19T18:22:19.073891', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 18:22:19: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7451, vector_size=100, alpha=0.025>', 'datetime': '2023-10-19T18:22:19.074096', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO - 18:22:20: collecting all words and their counts\n",
      "INFO - 18:22:20: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:22:20: PROGRESS: at sentence #10000, processed 241164 words, keeping 21647 word types\n",
      "INFO - 18:22:20: PROGRESS: at sentence #20000, processed 481958 words, keeping 31476 word types\n",
      "INFO - 18:22:20: collected 32305 word types from a corpus of 506449 raw words and 21046 sentences\n",
      "INFO - 18:22:20: Creating a fresh vocabulary\n",
      "INFO - 18:22:20: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 7451 unique words (23.06% of original 32305, drops 24854)', 'datetime': '2023-10-19T18:22:20.714456', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:20: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 469185 word corpus (92.64% of original 506449, drops 37264)', 'datetime': '2023-10-19T18:22:20.720683', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:20: deleting the raw counts dictionary of 32305 items\n",
      "INFO - 18:22:20: sample=0.001 downsamples 41 most-common words\n",
      "INFO - 18:22:20: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 366705.58609343914 word corpus (78.2%% of prior 469185)', 'datetime': '2023-10-19T18:22:20.749644', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:20: estimated required memory for 7451 words and 200 dimensions: 15647100 bytes\n",
      "INFO - 18:22:20: resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'vector_size': 100, 'window': 5, 'min_count': 5}\n",
      "Training Accuracy: 0.7120\n",
      "Dev Accuracy: 0.7151\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:22:20: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-10-19T18:22:20.834904', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "INFO - 18:22:20: Word2Vec lifecycle event {'msg': 'training model with 8 workers on 7451 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=10 shrink_windows=True', 'datetime': '2023-10-19T18:22:20.870967', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 18:22:21: EPOCH 0: training on 506449 raw words (366485 effective words) took 0.1s, 2511619 effective words/s\n",
      "INFO - 18:22:21: EPOCH 1: training on 506449 raw words (366592 effective words) took 0.2s, 2443631 effective words/s\n",
      "INFO - 18:22:21: EPOCH 2: training on 506449 raw words (366613 effective words) took 0.1s, 2602524 effective words/s\n",
      "INFO - 18:22:21: EPOCH 3: training on 506449 raw words (367287 effective words) took 0.1s, 2545393 effective words/s\n",
      "INFO - 18:22:21: EPOCH 4: training on 506449 raw words (366766 effective words) took 0.1s, 2520627 effective words/s\n",
      "INFO - 18:22:21: Word2Vec lifecycle event {'msg': 'training on 2532245 raw words (1833743 effective words) took 0.7s, 2455889 effective words/s', 'datetime': '2023-10-19T18:22:21.618275', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 18:22:21: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7451, vector_size=200, alpha=0.025>', 'datetime': '2023-10-19T18:22:21.618724', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'created'}\n",
      "INFO - 18:22:24: collecting all words and their counts\n",
      "INFO - 18:22:24: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "INFO - 18:22:24: PROGRESS: at sentence #10000, processed 241164 words, keeping 21647 word types\n",
      "INFO - 18:22:24: PROGRESS: at sentence #20000, processed 481958 words, keeping 31476 word types\n",
      "INFO - 18:22:24: collected 32305 word types from a corpus of 506449 raw words and 21046 sentences\n",
      "INFO - 18:22:24: Creating a fresh vocabulary\n",
      "INFO - 18:22:24: Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 7451 unique words (23.06% of original 32305, drops 24854)', 'datetime': '2023-10-19T18:22:24.805482', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:24: Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 469185 word corpus (92.64% of original 506449, drops 37264)', 'datetime': '2023-10-19T18:22:24.806677', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:24: deleting the raw counts dictionary of 32305 items\n",
      "INFO - 18:22:24: sample=0.001 downsamples 41 most-common words\n",
      "INFO - 18:22:24: Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 366705.58609343914 word corpus (78.2%% of prior 469185)', 'datetime': '2023-10-19T18:22:24.839557', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "INFO - 18:22:24: estimated required memory for 7451 words and 300 dimensions: 21607900 bytes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'vector_size': 200, 'window': 10, 'min_count': 5}\n",
      "Training Accuracy: 0.7090\n",
      "Dev Accuracy: 0.7070\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - 18:22:24: resetting layer weights\n",
      "INFO - 18:22:24: Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-10-19T18:22:24.928456', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'build_vocab'}\n",
      "INFO - 18:22:24: Word2Vec lifecycle event {'msg': 'training model with 8 workers on 7451 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=30 shrink_windows=True', 'datetime': '2023-10-19T18:22:24.929780', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 18:22:25: EPOCH 0: training on 506449 raw words (366541 effective words) took 0.2s, 1712789 effective words/s\n",
      "INFO - 18:22:25: EPOCH 1: training on 506449 raw words (366665 effective words) took 0.2s, 1736577 effective words/s\n",
      "INFO - 18:22:25: EPOCH 2: training on 506449 raw words (366341 effective words) took 0.2s, 1731313 effective words/s\n",
      "INFO - 18:22:25: EPOCH 3: training on 506449 raw words (366521 effective words) took 0.2s, 1697275 effective words/s\n",
      "INFO - 18:22:26: EPOCH 4: training on 506449 raw words (366813 effective words) took 0.2s, 1714529 effective words/s\n",
      "INFO - 18:22:26: Word2Vec lifecycle event {'msg': 'training on 2532245 raw words (1832881 effective words) took 1.1s, 1662597 effective words/s', 'datetime': '2023-10-19T18:22:26.036326', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'train'}\n",
      "INFO - 18:22:26: Word2Vec lifecycle event {'params': 'Word2Vec<vocab=7451, vector_size=300, alpha=0.025>', 'datetime': '2023-10-19T18:22:26.036735', 'gensim': '4.3.2', 'python': '3.10.12 (main, Jul  5 2023, 14:49:34) [Clang 14.0.6 ]', 'platform': 'macOS-12.5-arm64-arm-64bit', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: {'vector_size': 300, 'window': 30, 'min_count': 5}\n",
      "Training Accuracy: 0.7141\n",
      "Dev Accuracy: 0.7105\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def get_sentence_embedding3(sentence, w2v_model):\n",
    "    words = sentence.split()\n",
    "    embeddings = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word in w2v_model.wv:\n",
    "            embeddings.append(w2v_model.wv[word])\n",
    "    \n",
    "    if not embeddings:\n",
    "        return np.zeros(w2v_model.vector_size)  # Ensure embeddings are of the same size as the model's vector size\n",
    "    \n",
    "    sentence_embedding = np.mean(embeddings, axis=0)\n",
    "    \n",
    "    return sentence_embedding\n",
    "\n",
    "hyperparameters = [\n",
    "    {\"vector_size\": 100, \"window\": 5, \"min_count\": 5},\n",
    "    {\"vector_size\": 200, \"window\": 10, \"min_count\": 5},\n",
    "    {\"vector_size\": 300, \"window\": 30, \"min_count\": 5}\n",
    "]\n",
    "\n",
    "for params in hyperparameters:\n",
    "    w2v_model = Word2Vec(sentences=sentences, **params, workers=8)\n",
    "    \n",
    "    X_train = [get_sentence_embedding3(sentence, w2v_model) for sentence in train_text]\n",
    "    X_dev = [get_sentence_embedding3(sentence, w2v_model) for sentence in dev_text]\n",
    "    \n",
    "    X_train = np.array(X_train)\n",
    "    X_dev = np.array(X_dev)\n",
    "\n",
    "    assert np.all([len(x) == w2v_model.vector_size for x in X_train])\n",
    "    assert np.all([len(x) == w2v_model.vector_size for x in X_dev])\n",
    "    \n",
    "    clf = LogisticRegression(max_iter=1000).fit(X_train, train_label)\n",
    "    \n",
    "    train_acc = accuracy_score(train_label, clf.predict(X_train))\n",
    "    dev_acc = accuracy_score(dev_label, clf.predict(X_dev))\n",
    "    \n",
    "    print(f\"Parameters: {params}\")\n",
    "    print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "    print(f\"Dev Accuracy: {dev_acc:.4f}\")\n",
    "    print(\"-\"*40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scroll through the information above to see the exact differences in training and testing accuracy. It does not appear to change significantly, though changing the vector and window size definitely do change both training and dev accuracy. In order to find the best number for both, however, it would make sense to search through a much larger range of values using a tool such as gridsearch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the scores from the models and their input vectors from these three weeks. Compare the train and dev accuracy for these configurations: \n",
    "\n",
    "- NB\n",
    "- logistic regression (LR) with unigram count vectors\n",
    "- LR with unigram+bigram count vectors\n",
    "- LR with tfidf vectors\n",
    "- LR with pre-trained w2v vectors\n",
    "- LR with custom trained w2v vectors\n",
    "\n",
    "And analyze the accuracy results from train and dev data. What do you see in terms comparing different methods and input representations? What do you see in terms of train and dev accuracy trends? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB:\n",
    "Final train classification_rate: 0.82946878266654\n",
    "Final dev(test) classification_rate: 0.7329965021375826\n",
    "\n",
    "NaivesBayes is a probabalistic model and has a decent accuracy, but isn't as good when it comes to classification in general compared to logistic regerssion most of the time. It is a good baseline for comparison, however, for the following models.\n",
    "\n",
    "logistic regression (LR) with unigram count vectors:\n",
    "Final train classification_rate: 0.9753397320155849\n",
    "Final test classification_rate: 0.7271667314418966\n",
    "\n",
    "LR with unigram+bigram count vectors:\n",
    "Final train classification_rate: 0.9992872754917799\n",
    "Final test classification_rate: 0.743101438010105\n",
    "\n",
    "LR with unigram, and with unigram+bigram have high training accuracies but are lacking in their final classification rate. The unigram+bigram LR does a bit better than the NB model at 0.74 v. 0.73. The unigram only LR doesn't do as well as the NB model at 0.72 v. 0.73. The big gap between training and testing accuracy can indicate overfitting. This isn't that surprising because we didn't do any regularization.\n",
    "\n",
    "LR with tfidf vectors:\n",
    "Final train classification_rate: 0.9338591656371757\n",
    "Final test classification_rate: 0.759813447337738\n",
    "\n",
    "LR with TFIDF, even though it doesn't have as high of a training classification rate, actually has a higher final test classification rate than any of the above models. This indicates that this model is more generalizable than the above models.\n",
    "\n",
    "LR with pre-trained w2v vectors:\n",
    "Training Accuracy: 76.43%\n",
    "Development (test) Accuracy: 76.25%\n",
    "\n",
    "LR with custom trained w2v vectors:\n",
    "Training Accuracy: 75.76%\n",
    "Dev (test) Accuracy: 74.70%\n",
    "\n",
    "LR with w2v have training and testing accuracies that are much closer to each other than the other models. This indicates that there is less overfitting. The pre-trained w2v model performs a bit better than the custom trained w2v model, which is likely a result of the pre-trained model having a bigger training corpus. \n",
    "\n",
    "Looking at overal train and test accuracy trends, there is a lot of overfitting for the models that use big input vectors (unigram, unigram+bigram). You can easily see by the big difference in training classification rate and test classification rate. Models that have dense vector representation like w2v have more balance between train and test classification rates. This could be because w2v is better at capturing the essence of the text. Rather than including everything, it only captures the semantics, which reduces noise. Finally, the best model for classification out of the above are TFIDF and pre-trained w2v because they have the highest test/dev accuracy.\n",
    "\n",
    "The conclusion we can draw from this homework is that capturing semantics by transforming vectors is an essential part to creating a model that can classify generalized data. Having high training accuracy doesn't necessarily mean having high general classification rates, and can indicate overfitting. Dense vector representations like w2v and TFIDF seem to do a good job at making sure there is no overfitting by balancing model complexity and generalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
