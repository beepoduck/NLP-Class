# HW 11.1 Fine tuning BERT model for text classification

In this assignment, we will test the power of the BERT model on our old friend: the Triage data set. Can we use pretrained BERT models to fine tune on our small data set of Triage and get better accuracy than we previously did in HWs4.1-6.1?

We will play with the BERT classification code in Tensorflow. In tensorflow, there are several ways to load the text data, but the most common way is to load it with a defined folder structure using the `tf.keras.utils.text_dataset_from_directory ` (read through documentation here: https://www.tensorflow.org/tutorials/load_data/text). To use this function, you must store you text data in this folder sturcture: 
```
train/
...class_1/
......1.txt
......2.txt
...class_2/
......1.txt
......2.txt

test/
...class_1/
......1.txt
......2.txt
...class_2/
......1.txt
......2.txt
```
A big part of the real world job in NLP is to try to leverage existing code examples and try to fit your own data on it. In this assignment we will simulate that experience by giving you minimal instructions and you need to just "make it work". 

## Task 1: prepare data in your local computer

In order to use the text loading function above, we need to re-format the data into the format above. Note that each text file should only have one message / training example. 

After you wrote your data to the desired format, you can zip them up. For example:

`zip -r triage.zip . -x '**/.*' -x '**/__MACOSX'`

(on the command line terminal)

Then you should upload your zip file onto your BOX. In BOX, you should create a sharable link so that you can download it using code in your colab notebook. Note that you need to click into "link settings" to get a direct link to your data zip file. 

# Task 2: Fine tuning BERT

Head over to the Colab notebook https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb . Run through each cell and make sure you understand all the explanations of what they are doing. 

Your task is to fine tune the BERT model on your own Triage dataset. First you need to download your data set from BOX using either the `wget` command or following the code example. Then try to fine tune the BERT model on your Triage data set. Once you made it work, try:
(1) You can try 2-3 different BERT model versions ;
(2) Try adding another dense layer to the classifier; 
etc. and see if you can get a better test accuracy. 

# Notes

1. Note that in this case your train data will be further divided into train and val portions, and your test data will be reserved for testing, like before.

2. You may ignore the last part of exporting the model for inference. 

3. Again this homework will be running in Colab and finally you can download it and push it to github. When you are in Colab, you can check that you are indeed using GPU to train by selecting "change Runtime Type" in the Runtime menu and select the GPU option. 