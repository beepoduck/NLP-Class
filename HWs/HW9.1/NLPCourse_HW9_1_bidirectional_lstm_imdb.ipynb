{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnJ7J6x1SBYe"
      },
      "source": [
        "# Bidirectional LSTM on IMDB\n",
        "\n",
        "**Author:** [fchollet](https://twitter.com/fchollet)<br>\n",
        "**Date created:** 2020/05/03<br>\n",
        "**Last modified:** 2020/05/03<br>\n",
        "**Description:** Train a 2-layer bidirectional LSTM on the IMDB movie review sentiment classification dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf5Kgx1OSBYg"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hn4l_rG4SBYg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "max_features = 20000  # Only consider the top 20k words\n",
        "maxlen = 200  # Only consider the first 200 words of each movie review\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpHIhsYASBYh"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHk3RQfiSBYh",
        "outputId": "fc176755-ac72-4498-a592-90210ce56e98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 200)]             0         \n",
            "                                                                 \n",
            " embedding_9 (Embedding)     (None, 200, 128)          2560000   \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 25600)             0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 64)                1638464   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4202689 (16.03 MB)\n",
            "Trainable params: 4202689 (16.03 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Input for variable-length sequences of integers\n",
        "inputs = keras.Input(shape=(maxlen,), dtype=\"int32\")\n",
        "# Embed each integer in a 128-dimensional vector\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "\n",
        "# dense layers\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "\n",
        "# 1 bidirectional LSTMs instead\n",
        "# x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "\n",
        "# Add 2 bidirectional LSTMs\n",
        "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "# x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "\n",
        "# Add a classifier\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training data represented by model.summary() shows the layer types input_1, embedding, bidirectional, bidirectional_1 and dense. It shows the output shape of each and the number of parameters. At the bottom, there is a summary of the total parameters, the training parameters and non training parameters.\n",
        "\n",
        "The total parameter count is 2757761, the traininable parameter count is the same, and there are no non-trainable parameters.\n",
        "\n",
        "Changing the layer amount to only 1 leads to a change in the summary. Now, the listed layers are input_7, embedding_6, bidirectional_12 and dense_6. Total parameters are also less at 2658945."
      ],
      "metadata": {
        "id": "csEe222EXm4t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbUFzcxbSBYh"
      },
      "source": [
        "## Load the IMDB movie review sentiment data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp7trtTpSBYh",
        "outputId": "c26581f2-8513-4a5b-b5bd-d8bfbb13d6ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Training sequences\n",
            "25000 Validation sequences\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "print(len(x_train), \"Training sequences\")\n",
        "print(len(x_val), \"Validation sequences\")\n",
        "x_train = keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.utils.pad_sequences(x_val, maxlen=maxlen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7trccw6SBYi"
      },
      "source": [
        "## Train and evaluate the model\n",
        "\n",
        "You can use the trained model hosted on [Hugging Face Hub](https://huggingface.co/keras-io/bidirectional-lstm-imdb) and try the demo on [Hugging Face Spaces](https://huggingface.co/spaces/keras-io/bidirectional_lstm_imdb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrkgmI-GSBYi",
        "outputId": "0d21b34c-620e-4d30-a557-d988adf5176a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "782/782 [==============================] - 43s 49ms/step - loss: 0.3885 - accuracy: 0.8107 - val_loss: 0.3138 - val_accuracy: 0.8675\n",
            "Epoch 2/3\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.0636 - accuracy: 0.9792 - val_loss: 0.4678 - val_accuracy: 0.8482\n",
            "Epoch 3/3\n",
            "782/782 [==============================] - 9s 12ms/step - loss: 0.0059 - accuracy: 0.9984 - val_loss: 0.7774 - val_accuracy: 0.8508\n"
          ]
        }
      ],
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    batch_size = 32,\n",
        "    epochs=3,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[early_stopping]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default settings of batch size = 32, epoch = 2.\n",
        "Training accuracy: 0.8248, 0.8530\n",
        "Validation accuracy: 0.9230, 8514"
      ],
      "metadata": {
        "id": "ns_5HXAhXGdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**\n",
        "\n",
        "Looking at the results below, you can see the training accuracy increase throughout epochs for batch size = 16 and batch size = 64. This makes sense as each epoch allows the model to better fit the trianing data. However, validation accuracy actually doesn't always increase. It decreased from 0.8394 to 0.7962 for batch size = 16, which is a sign of overfitting. It increased from 0.8541 to 0.8657 for batch size = 64. This increase is very small, and therefore it doesn't necessarily mean the model is better.\n",
        "\n",
        "Batch size 16:\n",
        "\n",
        "training accuracy = 0.8204, 0.8969\n",
        "\n",
        "validation accuracy = 0.8394, 0.7962\n",
        "\n",
        "Batch size 64:\n",
        "\n",
        "training accuracy = 0.8222, 0.9137\n",
        "\n",
        "validation accuracy = 0.8541, 0.8657\n",
        "\n",
        "By having more epochs, the final training validation accuracy becomes very high as the model can fit very well to the training data. However, the final validation accuracy is actually lower than previous epochs. As we've seen in the first part of task 1, more epochs doesn't necessarily mean a higher validation score. Each epoch beyond the first one, can lead to overfitting. Therefore, more epochs can actually be a bad thing. One remedy for this is to use early stop, to prevent overfitting.\n",
        "\n",
        "Holding batch size = 32 and epoch = 5.\n",
        "\n",
        "Final training accuracy = 0.9713\n",
        "\n",
        "Final validation accuracy = 0.8618\n",
        "\n",
        "Best training accuracy is the 5th epoch 32 batch training, but what really matters is validation accuracy. The best validation accuracies were 0.8635 for a model with batch size 32 on the third epoch, and 0.8657 with batch size 64 on the second epoch. Since they're close, we'll stick with the third epoch of batch size 32."
      ],
      "metadata": {
        "id": "XJrCOQZaYU5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2**\n",
        "\n",
        "With only one layer, we know from class that the model will be less complex. However, this isn't necessarily a bad thing because it can lower training time and prevent overfitting if the data is actually not that complex. Having implemented that, we have the following:\n",
        "\n",
        "Training accuracy is the using 1 or 2 layers and batch size 32. With 1 layer, training accuracy was 0.9228 on the third epoch. With 2 layers, training accuracy was 0.9272 on the third epoch.\n",
        "\n",
        "Validation accuracy was 0.8635 with 2 layers, and 0.8580 with 1 layer. This difference is not significant for us to conclude that 2 layers is better."
      ],
      "metadata": {
        "id": "TO3o0A62iWgG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 3**\n",
        "\n",
        "After changing the LSTM model to use MLP with dense layers instead, the parameter count and layers have changed. Now, the layer types input_2, embeding_1, flatten_1, dense_1, dense_2, and dense_3. The total parameters are 4,202,689, which is more parameters than the original bidirectional lstm with 2 layers which had 2,757,761 parameters.\n",
        "\n",
        "It runs properly as I added a Flatten layer to convert the 2D output of the embedding layer into a 1D array suitable for input into the Dense layers. The Flatten layer will only work if your input sequences are of the same length. The purpose of flattening was to convert the multi-dimensional input into a single-dimensional vector, so it can be fed into a fully connected layer (Dense layer), which does not accept multi-dimensional data as input.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qL1WQ7b0lNp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wrap up**\n",
        "\n",
        "In order to test for the best hyperparameters and layer setup, I compared MLP to the original models. Looking only at our MLP model, it looks like early stopping combined with a batch size of 64 performed better than with a batch size of 32. However, the validation accuracy really didn't differ significantly enough for us to say one is better than the other.\n",
        "\n",
        "Trial 1: earling stopping lstm 2 layer, batch size 32.\n",
        "\n",
        "accuracy: 0.8278\n",
        "\n",
        "val_accuracy: 0.8686\n",
        "\n",
        "Trial 2: early stopping lstm 2 layer, batch size 64\n",
        "\n",
        "accuracy: 0.8302\n",
        "\n",
        "val_accuracy: 0.8631\n",
        "\n",
        "Trial 3: early stopping MLP, batch size 32\n",
        "\n",
        "\n",
        "accuracy: 0.8145\n",
        "\n",
        "val_accuracy: 0.8671\n",
        "\n",
        "Trial 4: early stopping MLP, batch size 64\n",
        "\n",
        "accuracy: 0.7888\n",
        "\n",
        "val_accuracy: 0.8705"
      ],
      "metadata": {
        "id": "6h6OsVY8Y38J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**\n",
        "\n",
        "To summarize what we've learned, I organized the performance of different models and hyperparameters:\n",
        "\n",
        "**Batch Sizes and Epochs:**\n",
        "- A batch size of 32 generally provided a good balance between training speed and model performance.\n",
        "- A batch size of 16 showed signs of slower learning and potentially noisier updates.\n",
        "- Larger batch sizes (64) did not consistently improve validation performance and could lead to faster overfitting.\n",
        "- Increasing the training to have more epochs (e.g. 5) without regularization led to overfitting.\n",
        "\n",
        "**Model Layout:**\n",
        "- Two-layer bidirectional LSTMs: Has the ability to capture complex patterns in sequence data but tended to overfit shown by the divergence of training and validation accuracy.\n",
        "- Single-layer bidirectional LSTMs: Performed comparably in terms of validation accuracy but with less overfitting compared to the two-layer one.\n",
        "- The MLP with dense layers: much higher training accuracy to near-perfect levels, but wasn't matched with much improvements in validation accuracy which suggests overfitting.\n",
        "\n",
        "**Things that could be improved:**\n",
        "- Adding regularization such as dropout or L1/L2 to address\n",
        "overfitting.\n",
        "  - HOWEVER!!! After adding dropout, it actually didn't seem like the validation accuracy improved by much, so it might need a combination of the different regularization techniques to do better.\n",
        "- Adding early stopping helped find the best validation accuracy faster, but the validation accuracy itself didn't improve.\n",
        "- Changing the learning rates could improve validation accuracy.\n",
        "- Data preprocessing could improve model generalization.\n",
        "- Cross-validation could better show model performance.\n",
        "\n",
        "**Best Performing Trials:**\n",
        "- The two-layer LSTM with early stopping and a batch size of 32 had the best initial validation accuracy, but showed overfitting by the third epoch.\n",
        "- MLP trained very fast and had high training accuracies but showed significant overfitting, as seen by the large difference between training and validation accuracy.\n",
        "\n",
        "**Organized Data:**\n",
        "\n",
        "| Model                          | Batch Size | Epochs | Max Training Accuracy | Max Validation Accuracy | Notes                                 |\n",
        "|--------------------------------|------------|--------|----------------------|-------------------------|---------------------------------------|\n",
        "| Default LSTM (2 layers)        | 32         | 2      | 92.30%               | 85.30%                  | Initial good val_accuracy             |\n",
        "| Default LSTM (2 layers)        | 16         | 2      | 89.69%               | 83.94%                  | Noisy updates, lower val_accuracy     |\n",
        "| Default LSTM (2 layers)        | 64         | 2      | 91.37%               | 86.57%                  | Best initial val_accuracy             |\n",
        "| LSTM (2 layers, extended)      | 32         | 5      | 97.13%               | 86.72%                  | Overfitting after 2 epochs            |\n",
        "| Single-layer LSTM              | 32         | 3      | 92.28%               | 86.40%                  | Comparable to two layers              |\n",
        "| MLP (2 dense layers)           | 32         | 3      | 99.70%               | 86.45%                  | Significant overfitting               |\n",
        "| Early Stopping LSTM (2 layers) | 32         | 3      | 95.55%               | 86.86%                  | Early stopping helps                  |\n",
        "| Early Stopping LSTM (2 layers) | 64         | 3      | 96.10%               | 86.31%                  | Slightly lower val_accuracy           |\n",
        "| Early Stopping MLP             | 32         | 3      | 99.74%               | 86.71%                  | Early stopping, still overfitting     |\n",
        "| Early Stopping MLP             | 64         | 3      | 99.91%               | 87.05%                  | Highest initial val_accuracy, overfit |"
      ],
      "metadata": {
        "id": "hNhCFIU_l7VD"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}