# HW9.1 Sentiment classification with LSTMs and MLPs

In this homework we will work with the tf.keras framework. To simplify things, we will use the keras built-in dataset of IMDB movie reviews. To use the Colab GPUs, let's run this whole exercise in Colab, and then download it to your laptop and push it to the github repo under your own branch, within a folder named HW9.1.

## Run Notebook on bi-LSTM movie review classification in Colab

We will get started with the code example found in the Keras documentation. Head to this page, and then open the notebook in Colab. 

https://keras.io/examples/nlp/bidirectional_lstm_imdb/

Run through all the cells and try to understand what each cell is doing. Especially pay attention to the output of ```model.summary()```. Look at the training data. How are they represented?

Record the parameter count of this model. Also record the final accuracy of the model. 

## Task 1: hyperparameters

Try change the batch size to 16 and 64. How does the accuracy change?

While holding the batch size 32, experiment with training the model longer with a few more epochs. How does final the accuracy change? 

Record the best accuracy and the settings you obtained them with. Use these settings going forward.

## Task 2: modify LSTM architecture

The original model has two layers of bi-directional LSTMs. Change it to only one and train again. How does the accuracy change? Write down any observations. 

## Task 3: use MLP (dense layers)
Now let's swap out the bi-LSTM layers with two dense layers with 64 hidden units. (refer to keras Functional API documentation for how to add a dense layer). Try to compile the model and look at the summary. How is it different from the LSTM model? How does the parameter count differ? 

Now try to train the MLP model. Does it run?

If it doesn't run, can you explain why by looking at the comparison between the two model summary outputs? 

Hint: in addition to adding two dense layers, you might need to also change the input dimension specification to maxlen, and do a Flatten operation between the embedding layer and the dense layer. 

Try making changes to the MLP network so that you can train a model with it too.

## Wrap up
Once you got the MLP training running, play around with it to get the best accuracy. Report the final accuracy and compare it with the previous models. 